# SEM Image Colorization
Colorize Scanning Electron Microscope (SEM) images automatically using Deep Learning & Computer Vision

### Results
Original (grayscale) images and their colorized versions produced by this project

![Sample1](./samples/Sample1.jpg)

![Sample2](./samples/Sample2.jpg)

![Sample3](./samples/Sample3.jpg)

# The colorization problem
SEM images are, by the nature of the electron microscopes, color absent. Colorization is a important step to any SEM image because it helps us humans to better interpret it since the human brain works better when colors are present. However, the colorization process is slow and manual, demanding time and effort from researchers. The goal of this project is to create an automatic way of colorization.

# How it works
A cGAN (Conditional Generative Adversarial Network), based on [Pix2Pix](https://arxiv.org/pdf/1611.07004), is applied to the Image Colorization problem.
There are two networks that work together in an adversarial game: 
* Generator: UNet-based with Resnet18 as backbone
* Discriminator: PatchGAN 

During the optimization process, the Discriminator is trained to better differentiate the real images and the generated ones by the Generator. The Generator, in the opposite way, is optimized to generate images that better fool the Discriminator. This adversarial game produces a dynamic training behaviour in which, at the end, the goal is to produce a Generator good enough that fools the Discriminator, so the image it produces are good-looking and convincing.

The colorization is made by leveraging the LAB color space instead of RGB. The original image (grayscale) corresponds to the L channel and the remaining A and B channels are generated by the Generator model. Then the L, A and B channels are concatenated and a LAB image is produced. 
  
### Colorization diagram
![Diagram](./colorization-diagram.png)

# Setup the Environment

If you're using uv:

`uv sync`

If you're using pip, create a venv with python >=3.10 and install the dependencies with:

`pip install -r requirements.txt`

# Steps to reproduce
The training happens in two stages. 

At first, the Generator is trained individually by being fine-tuned on the data using L1 loss as criterion. By doing so, it learns the context of the data and tune its parameters to better minimize the L1 loss between the generated outputs and the ground truth.

To pretrain the Generator, run the `pretrain.py` script with the desired hyperparameters. 

Then, the cGAN is trained to improve the Generator at its best. So the pretrained Generator is again fine-tuned from the pretrain's checkpoint.

To run the cGAN training, run:
`train_pix2pix.py`

# Run the Web app
![WebApp](./samples/webapp.png)

This is the recommended way to run the application, as it automatically sets up both the backend API and the frontend web app.

### 1. Build and Run the Containers:

From project's root directory, run the following command. The --build flag is only necessary the first time you run it or after you change dependencies.

`docker-compose up --build`

### 2. Access the Web App:

Once the containers are running, open your web browser and navigate to:

http://localhost:8501
 
### 3. Stopping the Application:

To stop the containers, run:
    
`docker-compose down`